<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="author" content="Тураев Тимур" />


<title>Seeds</title>

<script src="seeds_files/jquery-1.11.0/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link href="seeds_files/bootstrap-2.3.2/css/spacelab.min.css" rel="stylesheet" />
<link href="seeds_files/bootstrap-2.3.2/css/bootstrap-responsive.min.css" rel="stylesheet" />
<script src="seeds_files/bootstrap-2.3.2/js/bootstrap.min.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; background-color: #f8f8f8; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
pre, code { background-color: #f8f8f8; }
code > span.kw { color: #204a87; font-weight: bold; }
code > span.dt { color: #204a87; }
code > span.dv { color: #0000cf; }
code > span.bn { color: #0000cf; }
code > span.fl { color: #0000cf; }
code > span.ch { color: #4e9a06; }
code > span.st { color: #4e9a06; }
code > span.co { color: #8f5902; font-style: italic; }
code > span.ot { color: #8f5902; }
code > span.al { color: #ef2929; }
code > span.fu { color: #000000; }
code > span.er { font-weight: bold; }
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>



</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
</style>
<div class="container-fluid main-container">


<div id="header">
<h1 class="title">Seeds</h1>
<h4 class="author"><em>Тураев Тимур</em></h4>
<h4 class="date"><em>16 декабря 2014 г.</em></h4>
</div>


<pre><code>## Loading required package: RColorBrewer</code></pre>
<p>Загружаем наши данные и переименовываем столбцы так, как указано в комментариях к данным.</p>
<pre class="sourceCode r"><code class="sourceCode r">df &lt;-<span class="st"> </span><span class="kw">read.table</span>(<span class="st">&#39;data/seeds.txt&#39;</span>)
<span class="kw">names</span>(df) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;area&quot;</span>, <span class="st">&quot;perimeter&quot;</span>, <span class="st">&quot;compactness&quot;</span>, <span class="st">&quot;length&quot;</span>, <span class="st">&quot;width&quot;</span>, <span class="st">&quot;asymmetry&quot;</span>, <span class="st">&quot;grooveLength&quot;</span>, <span class="st">&quot;variety&quot;</span>)
df$variety &lt;-<span class="st"> </span><span class="kw">as.factor</span>(df$variety)
<span class="kw">summary</span>(df)</code></pre>
<pre><code>##       area         perimeter      compactness         length     
##  Min.   :10.59   Min.   :12.41   Min.   :0.8081   Min.   :4.899  
##  1st Qu.:12.27   1st Qu.:13.45   1st Qu.:0.8569   1st Qu.:5.262  
##  Median :14.36   Median :14.32   Median :0.8734   Median :5.524  
##  Mean   :14.85   Mean   :14.56   Mean   :0.8710   Mean   :5.629  
##  3rd Qu.:17.30   3rd Qu.:15.71   3rd Qu.:0.8878   3rd Qu.:5.980  
##  Max.   :21.18   Max.   :17.25   Max.   :0.9183   Max.   :6.675  
##      width         asymmetry       grooveLength   variety
##  Min.   :2.630   Min.   :0.7651   Min.   :4.519   1:70   
##  1st Qu.:2.944   1st Qu.:2.5615   1st Qu.:5.045   2:70   
##  Median :3.237   Median :3.5990   Median :5.223   3:70   
##  Mean   :3.259   Mean   :3.7002   Mean   :5.408          
##  3rd Qu.:3.562   3rd Qu.:4.7687   3rd Qu.:5.877          
##  Max.   :4.033   Max.   :8.4560   Max.   :6.550</code></pre>
<p>Построим всякие графики:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">splom</span>(~df, <span class="dt">data=</span>df,
      <span class="dt">upper.panel=</span>function(x, y, ...) { <span class="kw">panel.xyplot</span>(x, y, ...); <span class="kw">panel.loess</span>(x, y, ..., <span class="dt">col=</span><span class="st">&#39;red&#39;</span>) },
      <span class="dt">lower.panel=</span>function(x, y, ...) { },
      <span class="dt">pscale=</span><span class="dv">0</span>, <span class="dt">varname.cex=</span><span class="fl">1.4</span>, <span class="dt">par.settings=</span><span class="kw">simpleTheme</span>(<span class="dt">pch=</span><span class="dv">17</span>, <span class="dt">cex=</span><span class="fl">0.2</span>))</code></pre>
<div class="figure">
<img src="seeds_files/figure-html/unnamed-chunk-3-1.png" />
</div>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">corrplot.mixed</span>(<span class="kw">cor</span>(<span class="kw">subset</span>(df, <span class="dt">select=</span>-variety)), <span class="dt">tl.cex=</span><span class="fl">1.5</span>)</code></pre>
<div class="figure">
<img src="seeds_files/figure-html/unnamed-chunk-4-1.png" />
</div>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">marginal.plot</span>(df)</code></pre>
<div class="figure">
<img src="seeds_files/figure-html/unnamed-chunk-5-1.png" />
</div>
<p>Поделим выборку на обучающую и тестовую в отношении 2/1:</p>
<pre class="sourceCode r"><code class="sourceCode r">train.idx &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">nrow</span>(df), <span class="dt">size =</span> <span class="kw">nrow</span>(df) *<span class="st"> </span><span class="fl">0.6666</span>)
df.train &lt;-<span class="st"> </span>df[train.idx, ]
df.test &lt;-<span class="st"> </span>df[-train.idx, ]</code></pre>
<div id="lda" class="section level3">
<h3>LDA</h3>
<p>Установим функцию тестирования нашей модели (10-fold cross validation) и проверим test-train (выведем относительную ошибку)</p>
<pre class="sourceCode r"><code class="sourceCode r">check.testTrain.lda &lt;-<span class="st"> </span>function(f) {
  m.predicted &lt;-<span class="st"> </span><span class="kw">predict</span>(<span class="kw">lda</span>(f, <span class="dt">data =</span> df.train), df.test)
  <span class="kw">print</span>(<span class="kw">table</span>(m.predicted$class, df.test$variety))
  <span class="kw">mean</span>(m.predicted$class !=<span class="st"> </span>df.test$variety)
}

check.lda =<span class="st"> </span>function(f) {
  <span class="kw">print</span>(<span class="kw">lda</span>(f, df))
  t &lt;-<span class="st"> </span><span class="kw">tune</span>(lda, f, <span class="dt">data =</span> df, <span class="dt">predict.func =</span> function(...) <span class="kw">predict</span>(...)$class)
  <span class="kw">print</span>(<span class="kw">check.testTrain.lda</span>(f))
  t
}

<span class="kw">check.lda</span>(variety ~<span class="st"> </span>.)</code></pre>
<pre><code>## Call:
## lda(f, data = df)
## 
## Prior probabilities of groups:
##         1         2         3 
## 0.3333333 0.3333333 0.3333333 
## 
## Group means:
##       area perimeter compactness   length    width asymmetry grooveLength
## 1 14.33443  14.29429   0.8800700 5.508057 3.244629  2.667403     5.087214
## 2 18.33429  16.13571   0.8835171 6.148029 3.677414  3.644800     6.020600
## 3 11.87386  13.24786   0.8494086 5.229514 2.853771  4.788400     5.116400
## 
## Coefficients of linear discriminants:
##                      LD1         LD2
## area         -0.42377861   4.1953167
## perimeter     3.79919995  -8.5057958
## compactness   5.92772810 -86.9823024
## length       -5.98819597  -7.8306747
## width         0.03704822   0.7141043
## asymmetry    -0.04504722   0.3212538
## grooveLength  3.11807592   6.9138493
## 
## Proportion of trace:
##    LD1    LD2 
## 0.6814 0.3186 
##    
##      1  2  3
##   1 23  1  0
##   2  0 25  0
##   3  1  0 21
## [1] 0.02816901</code></pre>
<pre><code>## 
## Error estimation of &#39;lda&#39; using 10-fold cross validation: 0.03809524</code></pre>
<p>Надо бы избавиться в модели от зависимых параметров.</p>
<p>Area и perimeter превращаются по формуле (в датасете она есть в описании данных) в compactness. Одно из измерений ядра тоже можно выкинуть.</p>
<p>После всех проб и ошибок я выбрал такую модель, минимизирующую ошибку: оставим только длины зерна и “groove” (однажды я вообще получил полное совпадение)</p>
<pre class="sourceCode r"><code class="sourceCode r">model &lt;-<span class="st"> </span>variety ~<span class="st"> </span>length +<span class="st"> </span>grooveLength
<span class="kw">check.lda</span>(model)</code></pre>
<pre><code>## Call:
## lda(f, data = df)
## 
## Prior probabilities of groups:
##         1         2         3 
## 0.3333333 0.3333333 0.3333333 
## 
## Group means:
##     length grooveLength
## 1 5.508057     5.087214
## 2 6.148029     6.020600
## 3 5.229514     5.116400
## 
## Coefficients of linear discriminants:
##                     LD1       LD2
## length       -0.1184181 -9.901326
## grooveLength  4.4258155  8.297548
## 
## Proportion of trace:
##    LD1    LD2 
## 0.7006 0.2994 
##    
##      1  2  3
##   1 23  0  0
##   2  0 25  0
##   3  1  1 21
## [1] 0.02816901</code></pre>
<pre><code>## 
## Error estimation of &#39;lda&#39; using 10-fold cross validation: 0.03333333</code></pre>
<p>От запуска к запуску цифры все время разные, но раз модели предсказания почти одинаковые, почему бы не взять ту, которая проще :)</p>
</div>
<div id="naive-bayes" class="section level2">
<h2>Naive Bayes</h2>
<p>Снова тестовая функция:</p>
<pre class="sourceCode r"><code class="sourceCode r">check.testTrain.nb =<span class="st"> </span>function(f) {
  m.predicted &lt;-<span class="st"> </span><span class="kw">predict</span>(<span class="kw">naiveBayes</span>(f, <span class="dt">data =</span> df.train), df.test)
  <span class="kw">print</span>(<span class="kw">table</span>(m.predicted, df.test$variety))
  <span class="kw">mean</span>(m.predicted !=<span class="st"> </span>df.test$variety)
}

check.nb =<span class="st"> </span>function(f) {
  <span class="kw">print</span>(<span class="kw">naiveBayes</span>(f, df))
  t &lt;-<span class="st"> </span><span class="kw">tune</span>(naiveBayes, f, <span class="dt">data =</span> df)
  <span class="kw">print</span>(<span class="kw">check.testTrain.nb</span>(f))
  t
}

<span class="kw">check.nb</span>(<span class="kw">as.factor</span>(variety) ~<span class="st"> </span>.)</code></pre>
<pre><code>## 
## Naive Bayes Classifier for Discrete Predictors
## 
## Call:
## naiveBayes.default(x = X, y = Y, laplace = laplace)
## 
## A-priori probabilities:
## Y
##         1         2         3 
## 0.3333333 0.3333333 0.3333333 
## 
## Conditional probabilities:
##    area
## Y       [,1]      [,2]
##   1 14.33443 1.2157036
##   2 18.33429 1.4394963
##   3 11.87386 0.7230036
## 
##    perimeter
## Y       [,1]      [,2]
##   1 14.29429 0.5765831
##   2 16.13571 0.6169950
##   3 13.24786 0.3401956
## 
##    compactness
## Y        [,1]       [,2]
##   1 0.8800700 0.01619093
##   2 0.8835171 0.01550004
##   3 0.8494086 0.02175963
## 
##    length
## Y       [,1]      [,2]
##   1 5.508057 0.2315080
##   2 6.148029 0.2681911
##   3 5.229514 0.1380152
## 
##    width
## Y       [,1]      [,2]
##   1 3.244629 0.1776155
##   2 3.677414 0.1855391
##   3 2.853771 0.1475161
## 
##    asymmetry
## Y       [,1]     [,2]
##   1 2.667403 1.173901
##   2 3.644800 1.181868
##   3 4.788400 1.336465
## 
##    grooveLength
## Y       [,1]      [,2]
##   1 5.087214 0.2636987
##   2 6.020600 0.2539338
##   3 5.116400 0.1620683
## 
##            
## m.predicted  1  2  3
##           1 22  4  0
##           2  0 22  0
##           3  2  0 21
## [1] 0.08450704</code></pre>
<pre><code>## 
## Error estimation of &#39;naiveBayes&#39; using 10-fold cross validation: 0.1</code></pre>
<p>Попробуем ту же модель, что мы нашли для lda:</p>
<pre class="sourceCode r"><code class="sourceCode r">model &lt;-<span class="st"> </span><span class="kw">as.factor</span>(variety) ~<span class="st"> </span>length +<span class="st"> </span>grooveLength
<span class="kw">check.nb</span>(model)</code></pre>
<pre><code>## 
## Naive Bayes Classifier for Discrete Predictors
## 
## Call:
## naiveBayes.default(x = X, y = Y, laplace = laplace)
## 
## A-priori probabilities:
## Y
##         1         2         3 
## 0.3333333 0.3333333 0.3333333 
## 
## Conditional probabilities:
##    length
## Y       [,1]      [,2]
##   1 5.508057 0.2315080
##   2 6.148029 0.2681911
##   3 5.229514 0.1380152
## 
##    grooveLength
## Y       [,1]      [,2]
##   1 5.087214 0.2636987
##   2 6.020600 0.2539338
##   3 5.116400 0.1620683
## 
##            
## m.predicted  1  2  3
##           1 20  2  4
##           2  0 24  0
##           3  4  0 17
## [1] 0.1408451</code></pre>
<pre><code>## 
## Error estimation of &#39;naiveBayes&#39; using 10-fold cross validation: 0.1428571</code></pre>
<p>Не очень. Оказалось, что такая чуть лучше, но все равно не кардинально улучшает модель, да и от запуска к запуску числа разные.</p>
<pre class="sourceCode r"><code class="sourceCode r">model &lt;-<span class="st"> </span><span class="kw">as.factor</span>(variety) ~<span class="st"> </span>area +<span class="st"> </span>grooveLength
<span class="kw">check.nb</span>(model)</code></pre>
<pre><code>## 
## Naive Bayes Classifier for Discrete Predictors
## 
## Call:
## naiveBayes.default(x = X, y = Y, laplace = laplace)
## 
## A-priori probabilities:
## Y
##         1         2         3 
## 0.3333333 0.3333333 0.3333333 
## 
## Conditional probabilities:
##    area
## Y       [,1]      [,2]
##   1 14.33443 1.2157036
##   2 18.33429 1.4394963
##   3 11.87386 0.7230036
## 
##    grooveLength
## Y       [,1]      [,2]
##   1 5.087214 0.2636987
##   2 6.020600 0.2539338
##   3 5.116400 0.1620683
## 
##            
## m.predicted  1  2  3
##           1 21  1  2
##           2  0 25  0
##           3  3  0 19
## [1] 0.08450704</code></pre>
<pre><code>## 
## Error estimation of &#39;naiveBayes&#39; using 10-fold cross validation: 0.09047619</code></pre>
<p>Это, я думаю, происходит оттого, что данных не сказать что бы много, да и очень много зависимых параметров.</p>
</div>
<div id="multinomial-regression" class="section level2">
<h2>Multinomial regression</h2>
<p>Снова тестовые фукнции</p>
<pre class="sourceCode r"><code class="sourceCode r">check.testTrain.mr =<span class="st"> </span>function(f) {
  m.predicted &lt;-<span class="st"> </span><span class="kw">predict</span>(<span class="kw">multinom</span>(f, df.train, <span class="dt">maxit =</span> <span class="dv">2000</span>, <span class="dt">trace=</span><span class="ot">FALSE</span>), df.test)
  <span class="kw">print</span>(<span class="kw">table</span>(m.predicted, df.test$variety))
  <span class="kw">mean</span>(m.predicted !=<span class="st"> </span>df.test$variety)
}

check.mr =<span class="st"> </span>function(f) {
  (mr &lt;-<span class="st"> </span><span class="kw">multinom</span>(f, df, <span class="dt">maxit =</span> <span class="dv">2000</span>, <span class="dt">trace=</span><span class="ot">FALSE</span>))
  t &lt;-<span class="st"> </span><span class="kw">tune</span>(multinom, f, <span class="dt">data =</span> df, <span class="dt">maxit=</span><span class="dv">2000</span>, <span class="dt">trace=</span><span class="ot">FALSE</span>)
  <span class="kw">print</span>(<span class="kw">check.testTrain.mr</span>(f))
  t
}

<span class="kw">check.mr</span>(<span class="kw">as.factor</span>(variety) ~<span class="st"> </span>.)</code></pre>
<pre><code>##            
## m.predicted  1  2  3
##           1 23  0  1
##           2  0 26  0
##           3  1  0 20
## [1] 0.02816901</code></pre>
<pre><code>## 
## Error estimation of &#39;multinom&#39; using 10-fold cross validation: 0.02857143</code></pre>
<p>Даже такая полная модель дала очень маленькую ошибку. Может попробовать поискать модель попроще с ошибкой той же степени? Попробуем формулу LDA:</p>
<pre class="sourceCode r"><code class="sourceCode r">model &lt;-<span class="st"> </span><span class="kw">as.factor</span>(variety) ~<span class="st"> </span>length +<span class="st"> </span>grooveLength
<span class="kw">check.mr</span>(model)</code></pre>
<pre><code>##            
## m.predicted  1  2  3
##           1 23  0  1
##           2  0 25  0
##           3  1  1 20
## [1] 0.04225352</code></pre>
<pre><code>## 
## Error estimation of &#39;multinom&#39; using 10-fold cross validation: 0.04285714</code></pre>
<p>Ошибка чуть больше.</p>
<p>Или из Байеса:</p>
<pre class="sourceCode r"><code class="sourceCode r">model &lt;-<span class="st"> </span><span class="kw">as.factor</span>(variety) ~<span class="st"> </span>area +<span class="st"> </span>grooveLength
<span class="kw">check.mr</span>(model)</code></pre>
<pre><code>##            
## m.predicted  1  2  3
##           1 22  3  0
##           2  0 23  0
##           3  2  0 21
## [1] 0.07042254</code></pre>
<pre><code>## 
## Error estimation of &#39;multinom&#39; using 10-fold cross validation: 0.07142857</code></pre>
<p>Еще увеличилась.</p>
<p>В целом, ничего лучше начальной самой общей модели мне найти не удалось, LDA-формула оказалась ближе всех, да и модель вышла довольно простая.</p>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
